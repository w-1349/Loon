#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import re
from datetime import datetime, timedelta

RULE_SOURCES = [
    {"name": "AdRules", "url": "https://raw.githubusercontent.com/Cats-Team/AdRules/main/adrules.list"},
    {"name": "anti-ad", "url": "https://anti-ad.net/surge2.txt"}
]
OUTPUT_FILE = "Loon_rules.txt"

def get_beijing_time():
    return (datetime.utcnow() + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S')

def is_valid_domain(domain):
    """ä¸¥æ ¼çš„åŸŸåéªŒè¯"""
    if not domain or len(domain) > 253:
        return False
    if not re.match(r'^[a-z0-9\-\.]+$', domain):
        return False
    if '..' in domain or domain.startswith('.') or domain.endswith('.'):
        return False
    labels = domain.split('.')
    if len(labels) < 2:
        return False
    for label in labels:
        if not 1 <= len(label) <= 63:
            return False
        if label.startswith('-') or label.endswith('-'):
            return False
    if labels[-1].isdigit():
        return False
    return True

def process_line(line):
    line = line.strip()
    if not line or line.startswith(('!', '#', '[')):
        return None

    upper = line.upper()
    if upper.startswith("DOMAIN-SUFFIX,"):
        return line
    if upper.startswith("DOMAIN,"):
        return line

    # .å¼€å¤´ -> DOMAIN-SUFFIX
    if line.startswith('.'):
        domain = line[1:].lower()
        if is_valid_domain(domain):
            return f"DOMAIN-SUFFIX,{domain}"
        return None

    # çº¯åŸŸå -> DOMAIN
    domain = line.lower()
    if is_valid_domain(domain):
        return f"DOMAIN,{domain}"
    return None

def remove_redundant_suffix_rules(rules):
    """
    ç§»é™¤è¢«å…¶ä»– DOMAIN-SUFFIX è§„åˆ™åŒ…å«çš„å†—ä½™è§„åˆ™
    ä¾‹å¦‚: baidu.com åŒ…å« 1.baidu.com, www.baidu.com ç­‰
    """
    # åˆ†ç¦» DOMAIN å’Œ DOMAIN-SUFFIX
    domain_rules = []
    suffix_domains = []  # åªå­˜åŸŸåéƒ¨åˆ†
    
    for rule in rules:
        upper = rule.upper()
        if upper.startswith("DOMAIN-SUFFIX,"):
            # æå–åŸŸåéƒ¨åˆ†
            domain = rule[14:].strip().lower()  # len("DOMAIN-SUFFIX,") = 14
            suffix_domains.append((domain, rule))
        elif upper.startswith("DOMAIN,"):
            domain_rules.append(rule)
    
    # æŒ‰åŸŸåå±‚çº§æ’åºï¼ˆçŸ­çš„åœ¨å‰ï¼Œé•¿çš„åœ¨åï¼‰
    # è¿™æ ·å…ˆæ£€æŸ¥çŸ­åŸŸåï¼Œå¯ä»¥æ›´å¿«æ’é™¤é•¿åŸŸå
    suffix_domains.sort(key=lambda x: len(x[0].split('.')))
    
    # ä½¿ç”¨ Trie æˆ–ç®€å•é›†åˆæ¥åˆ¤æ–­åŒ…å«å…³ç³»
    # ä¼˜åŒ–ï¼šå°†åŸŸåæŒ‰åç¼€æ ‘ç»“æ„ç»„ç»‡
    kept_domains = set()
    redundant = []
    kept_rules = []
    
    for domain, rule in suffix_domains:
        # æ£€æŸ¥æ˜¯å¦è¢«å·²æœ‰çš„æŸä¸ªåŸŸååŒ…å«
        # ä¾‹å¦‚ domain = "1.baidu.com", æ£€æŸ¥æ˜¯å¦æœ‰ "baidu.com" æˆ– "com" å·²è¢«ä¿ç•™
        parts = domain.split('.')
        is_redundant = False
        
        # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„åç¼€
        for i in range(len(parts) - 1):  # è‡³å°‘ä¿ç•™ä¸€çº§ï¼Œä¸æ£€æŸ¥ç©ºåç¼€
            suffix = '.'.join(parts[i:])
            if suffix in kept_domains:
                is_redundant = True
                redundant.append((domain, suffix))
                break
        
        if not is_redundant:
            kept_domains.add(domain)
            kept_rules.append(rule)
    
    print(f"   DOMAIN-SUFFIX å»é‡: {len(suffix_domains)} -> {len(kept_rules)} (ç§»é™¤ {len(redundant)} æ¡)")
    if redundant[:5]:
        print(f"   å†—ä½™ç¤ºä¾‹: {redundant[:3]}")
    
    return kept_rules, domain_rules, redundant

def main():
    all_rules = []
    source_stats = []
    
    print(f"[{get_beijing_time()}] ğŸš€ å¯åŠ¨æ™ºèƒ½å»é‡æ¨¡å¼...")
    print("=" * 60)

    headers = {'User-Agent': 'Mozilla/5.0 (compatible; RuleFetcher/1.0)'}

    for src in RULE_SOURCES:
        try:
            print(f"ğŸ“¥ æ‹‰å–: {src['name']}...")
            resp = requests.get(src['url'], timeout=30, headers=headers)
            resp.raise_for_status()
            
            lines = resp.text.splitlines()
            processed = [r for r in (process_line(l) for l in lines) if r]
            unique = list(dict.fromkeys(processed))
            
            print(f"   åŸå§‹: {len(lines)} | æœ‰æ•ˆ: {len(unique)}")
            source_stats.append({
                "name": src['name'], 
                "raw": len(lines), 
                "valid": len(unique)
            })
            all_rules.extend(unique)
            print(f"âœ… å®Œæˆ\n")

        except Exception as e:
            print(f"âŒ å¤±è´¥: {e}\n")

    print("=" * 60)
    print("ğŸ”„ æ‰§è¡Œåç¼€åŒ…å«å»é‡...")
    
    # å…¨å±€å»é‡
    seen = set()
    unique_rules = []
    for r in all_rules:
        if r not in seen:
            seen.add(r)
            unique_rules.append(r)
    
    print(f"   åŸºç¡€å»é‡å: {len(unique_rules)}")
    
    # æ‰§è¡Œæ™ºèƒ½åç¼€å»é‡
    suffix_rules, domain_rules, redundant = remove_redundant_suffix_rules(unique_rules)
    
    # æœ€ç»ˆæ’åºï¼šDOMAIN-SUFFIX åœ¨å‰ï¼ŒæŒ‰å­—æ¯åº
    suffix_rules.sort()
    domain_rules.sort()
    final_rules = suffix_rules + domain_rules
    
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"   æ€»è§„åˆ™: {len(final_rules)}")
    print(f"   â”œâ”€ DOMAIN-SUFFIX: {len(suffix_rules)}")
    print(f"   â””â”€ DOMAIN: {len(domain_rules)}")

    # æ„å»ºæ–‡ä»¶å¤´
    header = [
        f"# Loon åŸŸåé›†åˆ (Smart Dedup)",
        f"# ç”Ÿæˆæ—¶é—´: {get_beijing_time()}",
        f"# ç»Ÿè®¡: æ€»è®¡ {len(final_rules)} | DOMAIN-SUFFIX {len(suffix_rules)} | DOMAIN {len(domain_rules)}",
        f"# ä¼˜åŒ–: ç§»é™¤ {len(redundant)} æ¡è¢«åŒ…å«çš„å†—ä½™åç¼€è§„åˆ™",
        "# " + "=" * 58
    ]
    for s in source_stats:
        header.append(f"# æº: {s['name']} | åŸå§‹ {s['raw']} | æå– {s['valid']}")
    header.append("# " + "=" * 58)

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write('\n'.join(header) + '\n\n')
        f.write('\n'.join(final_rules))

    print(f"\nğŸ’¾ å·²ä¿å­˜: {OUTPUT_FILE}")
    print(f"[{get_beijing_time()}] ğŸ‰ å®Œæˆ!")

if __name__ == "__main__":
    main()
