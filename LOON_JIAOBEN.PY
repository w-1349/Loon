#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Loon åŸŸåé›†åˆè‡ªåŠ¨åŒ–åˆå¹¶å·¥å…· - çº¯å‡€åŸŸåç‰ˆ
âœ… ç§»é™¤å‰ç¼€ç‚¹ï¼Œä¿æŒæ–‡ä»¶æ¸…çˆ½
ğŸ“Š åŒ…å«ï¼šåŸå§‹ç»Ÿè®¡ã€æœ‰æ•ˆæå–ã€æºå†…å»é‡ã€å­åŸŸå‹ç¼©ã€è·¨æºå»é‡
"""

import requests
import re
from datetime import datetime, timedelta
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# â€”â€” æ ¸å¿ƒé…ç½®åŒº â€”â€”
RULE_SOURCES = [
    {"name": "AdRules", "url": "https://adrules.top/adrules.list"},
    {"name": "anti-ad", "url": "https://anti-ad.net/surge2.txt"}
]
OUTPUT_FILE = "Loon_rules.txt"
# è‡ªåŠ¨é€‚é…ä½ çš„ä»“åº“åœ°å€
SUBSCRIBE_URL = "https://raw.githubusercontent.com/ddcm1349/Loon/main/Loon_rules.txt"

def create_retry_session():
    """åˆ›å»ºç¨³å®šè¯·æ±‚ä¼šè¯ï¼Œé˜²æ­¢æ‹‰å–æ—¶ç½‘ç»œæ³¢åŠ¨"""
    session = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    session.mount("https://", HTTPAdapter(max_retries=retry))
    return session

def clean_domain(line):
    """
    é€šç”¨æ¸…æ´—é€»è¾‘ï¼š
    ä» DOMAIN, / DOMAIN-SUFFIX, / || / . ä¸­ç²¾å‡†æŠ å‡ºåŸŸåä¸»ä½“
    """
    line = line.strip()
    if not line or line.startswith(('#', '!', '[')): 
        return None

    # 1. å¤„ç†é€—å·åˆ†éš”æ ¼å¼ (é’ˆå¯¹ AdRules çš„ DOMAIN/DOMAIN-SUFFIX)
    if ',' in line:
        parts = line.split(',')
        for part in parts:
            p = part.strip()
            # åŒ…å«ç‚¹ä¸”ä¸æ˜¯æŒ‡ä»¤ï¼ˆå…¨å¤§å†™ï¼‰åˆ™ä¸ºåŸŸå
            if '.' in p and not p.isupper() and not p.startswith('/'):
                line = p
                break
    
    # 2. ç§»é™¤å¸¸è§çš„ Adblock è¯­æ³•å’Œå‰ç¼€ç¬¦å·
    if line.startswith('||'):
        line = line[2:].split('^')[0]
    
    # 3. ç»Ÿä¸€å»é™¤å‰ç¼€ç‚¹ï¼Œå¹¶å…¨éƒ¨è½¬ä¸ºå°å†™
    line = line.lstrip('.').lower().split('/')[0].split('^')[0]
    
    # 4. åˆæ³•æ€§æ­£åˆ™éªŒè¯
    if '.' in line and re.match(r'^[a-z0-9\-\.]+$', line):
        return line
    return None

def compress_domains(domain_list):
    """
    å­åŸŸåå†—ä½™å‹ç¼©é€»è¾‘ï¼š
    å³ä½¿è¾“å‡ºä¸å¸¦ç‚¹ï¼Œé€»è¾‘ä¾ç„¶ä¿ç•™ï¼šæœ‰ google.com å°±ä¸ç•™ ads.google.com
    """
    sorted_domains = sorted(list(domain_list), key=len)
    final_list = []
    redundant_count = 0
    for domain in sorted_domains:
        is_redundant = False
        for root in final_list:
            if domain.endswith('.' + root) or domain == root:
                is_redundant = True
                redundant_count += 1
                break
        if not is_redundant:
            final_list.append(domain)
    return final_list, redundant_count

def main():
    session = create_retry_session()
    source_stats = []
    all_valid_domains = []
    global_raw_total = 0

    print("="*50 + "\nğŸš€ å¼€å§‹æ‰§è¡Œ Loon è§„åˆ™çº¯å‡€ç‰ˆåˆå¹¶\n" + "="*50)

    for src in RULE_SOURCES:
        try:
            print(f"ğŸ“¥ æ­£åœ¨æ‹‰å–ã€{src['name']}ã€‘...")
            resp = session.get(src['url'], timeout=30)
            lines = resp.text.splitlines()
            
            raw_count = len(lines)
            valid_list = [d for d in (clean_domain(l) for l in lines) if d]
            
            # æºå†…å»é‡ç»Ÿè®¡
            unique_src = list(dict.fromkeys(valid_list))
            inner_dedup = len(valid_list) - len(unique_src)
            
            source_stats.append({
                "name": src['name'],
                "url": src['url'],
                "raw": raw_count,
                "valid": len(valid_list),
                "unique": len(unique_src),
                "inner_dedup": inner_dedup
            })
            
            all_valid_domains.extend(unique_src)
            global_raw_total += raw_count
            print(f"âœ… ã€{src['name']}ã€‘å·²æå–: {len(unique_src)} æ¡æœ‰æ•ˆåŸŸå")

        except Exception as e:
            print(f"âŒ ã€{src['name']}ã€‘æ‹‰å–å¤±è´¥: {e}")

    # å…¨å±€å»é‡ä¸å†—ä½™å‹ç¼©
    unique_global = list(dict.fromkeys(all_valid_domains))
    cross_dedup = len(all_valid_domains) - len(unique_global)
    final_domains, compressed_count = compress_domains(unique_global)
    
    # åŒ—äº¬æ—¶é—´è·å–
    beijing_time = (datetime.utcnow() + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S')

    # â€”â€” æ„å»ºè¯¦ç»†æŠ¥è¡¨å¤´ â€”â€”
    header = [
        f"# Loon åŸŸåé›†åˆ (DOMAIN-SET) çº¯å‡€åˆå¹¶ç‰ˆ",
        f"# ç”Ÿæˆæ—¶é—´ (åŒ—äº¬æ—¶é—´) : {beijing_time}",
        f"# è§„åˆ™æºåœ°å€:",
    ]
    for i, s in enumerate(source_stats, 1):
        header.append(f"#   {i}. {s['name']} è§„åˆ™: {s['url']}")
    header.append(f"#   æœ¬è§„åˆ™è®¢é˜…åœ°å€: {SUBSCRIBE_URL}")
    header.append("# " + "="*72)
    
    header.append("# ã€å„è§„åˆ™æºå•ç‹¬ç»Ÿè®¡ã€‘")
    for i, s in enumerate(source_stats, 1):
        header.append(f"#   {i}. {s['name']} | åŸå§‹è¡Œ: {s['raw']} | æœ‰æ•ˆæå–: {s['valid']} | æºå†…å»é‡: {s['inner_dedup']}")
    header.append("# " + "="*72)

    header.append("# ã€å…¨å±€åˆå¹¶ç»Ÿè®¡ã€‘")
    header.append(f"# åŸå§‹æ€»è¡Œæ•°æ±‡æ€»: {global_raw_total} æ¡")
    header.append(f"# è·¨æºé‡å¤å»é‡æ•°: {cross_dedup} æ¡ | å­åŸŸå†—ä½™å‹ç¼©æ•°: {compressed_count} æ¡")
    header.append(f"# æœ€ç»ˆå…¨å±€ä¿ç•™æ€»æ•°: {len(final_domains)} æ¡")
    header.append("# " + "="*72 + "\n")

    # â€”â€” å†™å…¥æ–‡ä»¶ï¼šä¸å†æ·»åŠ å‰ç¼€ç‚¹ â€”â€”
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write('\n'.join(header))
        # ç›´æ¥æŒ‰å­—æ¯æ’åºå†™å…¥çº¯åŸŸå
        f.write('\n'.join(sorted(final_domains)))

    print(f"\nğŸ“Š ä»»åŠ¡åœ†æ»¡å®Œæˆï¼æœ€ç»ˆç”Ÿæˆè§„åˆ™: {len(final_domains)} æ¡")

if __name__ == "__main__":
    main()
