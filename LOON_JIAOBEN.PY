#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import re
from datetime import datetime, timedelta

RULE_SOURCES = [
    {"name": "AdRules", "url": "https://raw.githubusercontent.com/Cats-Team/AdRules/main/adrules.list"},
    {"name": "anti-ad", "url": "https://anti-ad.net/surge2.txt"}
]
OUTPUT_FILE = "Loon_rules.txt"

def get_beijing_time():
    return (datetime.utcnow() + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S')

def is_valid_domain(domain):
    """ä¸¥æ ¼çš„åŸŸåéªŒè¯"""
    if not domain or len(domain) > 253:
        return False
    if not re.match(r'^[a-z0-9\-\.]+$', domain):
        return False
    if '..' in domain or domain.startswith('.') or domain.endswith('.'):
        return False
    labels = domain.split('.')
    if len(labels) < 2:
        return False
    for label in labels:
        if not 1 <= len(label) <= 63:
            return False
        if label.startswith('-') or label.endswith('-'):
            return False
    if labels[-1].isdigit():
        return False
    return True

def process_adrules_line(line):
    """AdRules æºï¼šåŸç”Ÿ Loon æ ¼å¼ï¼Œç›´æ¥ä¿ç•™"""
    line = line.strip()
    if not line or line.startswith(('#', '!', '[')):
        return None
    
    upper = line.upper()
    if upper.startswith(('DOMAIN,', 'DOMAIN-SUFFIX,', 'DOMAIN-KEYWORD,', 'DOMAIN-SET,')):
        return line
    
    return None

def process_antiad_line(line):
    """anti-ad æºï¼šéœ€è¦è½¬æ¢"""
    line = line.strip()
    if not line or line.startswith(('#', '!', '[')):
        return None
    
    upper = line.upper()
    if upper.startswith(('DOMAIN,', 'DOMAIN-SUFFIX,')):
        return line
    
    if line.startswith('.'):
        domain = line[1:].lower()
        if is_valid_domain(domain):
            return f"DOMAIN-SUFFIX,{domain}"
        return None
    
    domain = line.lower()
    if is_valid_domain(domain):
        return f"DOMAIN,{domain}"
    
    return None

def dedup_rules(rules):
    """
    ä¸¤æ­¥å…¨å±€å»é‡ï¼š
    1. å®Œå…¨ç›¸åŒå»é‡ï¼ˆè·¨æºï¼‰
    2. åŒ…å«å…³ç³»å»é‡ï¼ˆDOMAIN è¢« SUFFIX åŒ…å«ï¼ŒSUFFIX è¢« SUFFIX åŒ…å«ï¼‰
    """
    
    # ç¬¬é›¶æ­¥ï¼šå…¨å±€å®Œå…¨ç›¸åŒå»é‡
    unique_rules = list(dict.fromkeys(rules))
    dup_removed = len(rules) - len(unique_rules)
    if dup_removed > 0:
        print(f"   å®Œå…¨ç›¸åŒå»é‡: {len(rules)} -> {len(unique_rules)} (ç§»é™¤ {dup_removed} æ¡)")
    rules = unique_rules
    
    # ç¬¬ä¸€æ­¥ï¼šåˆ†ç±»æ”¶é›†
    domain_rules = []      # (åŸŸå, åŸè§„åˆ™)
    suffix_rules = []      # (åŸŸå, åŸè§„åˆ™)
    other_rules = []
    
    for rule in rules:
        upper = rule.upper()
        if upper.startswith('DOMAIN,'):
            domain = rule[7:].strip().lower()
            domain_rules.append((domain, rule))
        elif upper.startswith('DOMAIN-SUFFIX,'):
            domain = rule[14:].strip().lower()
            suffix_rules.append((domain, rule))
        else:
            other_rules.append(rule)
    
    # ç¬¬äºŒæ­¥ï¼šDOMAIN è¢« SUFFIX åŒ…å«å»é‡
    suffix_domains = set(d for d, _ in suffix_rules)
    
    final_domain_rules = []
    removed_domain_count = 0
    removed_domain_examples = []
    
    for domain, rule in domain_rules:
        parts = domain.split('.')
        is_covered = False
        cover_by = None
        
        for i in range(len(parts)):
            suffix = '.'.join(parts[i:])
            if suffix in suffix_domains:
                is_covered = True
                cover_by = suffix
                break
        
        if is_covered:
            removed_domain_count += 1
            if len(removed_domain_examples) < 3:
                removed_domain_examples.append(f"{domain}âŠ‚{cover_by}")
        else:
            final_domain_rules.append(rule)
    
    print(f"   DOMAIN: {len(domain_rules)} -> {len(final_domain_rules)} (ç§»é™¤ {removed_domain_count} æ¡è¢«SUFFIXåŒ…å«çš„)")
    if removed_domain_examples:
        print(f"      ç¤ºä¾‹: {', '.join(removed_domain_examples)}")
    
    # ç¬¬ä¸‰æ­¥ï¼šSUFFIX å†…éƒ¨åŒ…å«å»é‡
    suffix_rules.sort(key=lambda x: len(x[0].split('.')))
    
    kept_suffix_domains = set()
    final_suffix_rules = []
    redundant_suffix_count = 0
    redundant_suffix_examples = []
    
    for domain, rule in suffix_rules:
        parts = domain.split('.')
        is_redundant = False
        cover_by = None
        
        for i in range(1, len(parts)):
            suffix = '.'.join(parts[i:])
            if suffix in kept_suffix_domains:
                is_redundant = True
                cover_by = suffix
                break
        
        if is_redundant:
            redundant_suffix_count += 1
            if len(redundant_suffix_examples) < 3:
                redundant_suffix_examples.append(f"{domain}âŠ‚{cover_by}")
        else:
            kept_suffix_domains.add(domain)
            final_suffix_rules.append(rule)
    
    print(f"   DOMAIN-SUFFIX: {len(suffix_rules)} -> {len(final_suffix_rules)} (ç§»é™¤ {redundant_suffix_count} æ¡)")
    if redundant_suffix_examples:
        print(f"      ç¤ºä¾‹: {', '.join(redundant_suffix_examples)}")
    
    # åˆå¹¶ç»“æœ
    final_rules = final_suffix_rules + final_domain_rules + other_rules
    total_removed = dup_removed + removed_domain_count + redundant_suffix_count
    
    return final_rules, len(final_suffix_rules), len(final_domain_rules), total_removed

def main():
    print(f"[{get_beijing_time()}] ğŸš€ å¯åŠ¨è§„åˆ™æŠ“å–...")
    print("=" * 60, flush=True)

    all_rules = []
    source_stats = []
    
    headers = {'User-Agent': 'Mozilla/5.0 (compatible; RuleFetcher/1.0)'}

    for src in RULE_SOURCES:
        try:
            print(f"ğŸ“¥ æ‹‰å–: {src['name']}...", flush=True)
            resp = requests.get(src['url'], timeout=30, headers=headers)
            resp.raise_for_status()
            
            lines = resp.text.splitlines()
            
            if src['name'] == 'AdRules':
                processed = [r for r in (process_adrules_line(l) for l in lines) if r]
            else:
                processed = [r for r in (process_antiad_line(l) for l in lines) if r]
            
            # å•æºå»é‡
            unique_processed = list(dict.fromkeys(processed))
            
            suffix_raw = sum(1 for r in unique_processed if r.upper().startswith('DOMAIN-SUFFIX,'))
            domain_raw = sum(1 for r in unique_processed if r.upper().startswith('DOMAIN,'))
            
            print(f"   åŸå§‹: {len(lines)} | æå–: {len(unique_processed)} (S:{suffix_raw}/D:{domain_raw})", flush=True)
            
            source_stats.append({
                "name": src['name'], 
                "raw": len(lines), 
                "valid": len(unique_processed),
                "suffix": suffix_raw,
                "domain": domain_raw
            })
            all_rules.extend(unique_processed)
            print(f"âœ… å®Œæˆ", flush=True)

        except Exception as e:
            print(f"âŒ å¤±è´¥: {e}", flush=True)
            import traceback
            traceback.print_exc()

    print("=" * 60, flush=True)
    print(f"ğŸ”„ å…¨å±€å»é‡ä¼˜åŒ–ï¼ˆæ€»è®¡ {len(all_rules)} æ¡ï¼‰...", flush=True)
    
    # æ‰§è¡Œå»é‡
    final_rules, final_suffix, final_domain, total_removed = dedup_rules(all_rules)
    
    # æ’åºè¾“å‡º
    suffix_part = sorted([r for r in final_rules if r.upper().startswith('DOMAIN-SUFFIX,')])
    domain_part = sorted([r for r in final_rules if r.upper().startswith('DOMAIN,')])
    other_part = sorted([r for r in final_rules if not r.upper().startswith(('DOMAIN,', 'DOMAIN-SUFFIX,'))])
    final_rules = suffix_part + domain_part + other_part
    
    print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:", flush=True)
    print(f"   æ€»è®¡: {len(final_rules)} æ¡", flush=True)
    print(f"   â”œâ”€ DOMAIN-SUFFIX: {final_suffix}", flush=True)
    print(f"   â””â”€ DOMAIN: {final_domain}", flush=True)

    # æ„å»ºæ–‡ä»¶å¤´
    header = [
        f"# Loon åŸŸåé›†åˆ (Smart Dedup)",
        f"# ç”Ÿæˆæ—¶é—´: {get_beijing_time()}",
        f"# ç»Ÿè®¡: æ€»è®¡ {len(final_rules)} | DOMAIN-SUFFIX {final_suffix} | DOMAIN {final_domain}",
        f"# ä¼˜åŒ–: å…±ç§»é™¤ {total_removed} æ¡å†—ä½™è§„åˆ™",
        "# " + "=" * 58
    ]
    for s in source_stats:
        header.append(f"# æº: {s['name']} | åŸå§‹ {s['raw']} | æå– {s['valid']} (S:{s['suffix']}/D:{s['domain']})")
    header.append("# " + "=" * 58)

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write('\n'.join(header) + '\n\n')
        f.write('\n'.join(final_rules))

    print(f"\nğŸ’¾ å·²ä¿å­˜: {OUTPUT_FILE}", flush=True)
    print(f"[{get_beijing_time()}] ğŸ‰ å®Œæˆ!", flush=True)

if __name__ == "__main__":
    main()
