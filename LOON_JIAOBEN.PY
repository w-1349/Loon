#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests  # å¯¼å…¥ç½‘ç»œè¯·æ±‚åº“ï¼Œç”¨äºä¸‹è½½è¿œç¨‹è§„åˆ™æ–‡ä»¶
import re        # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“ï¼Œç”¨äºç²¾å‡†åŒ¹é…åŸŸåå­—ç¬¦
from datetime import datetime, timedelta  # å¯¼å…¥æ—¶é—´æ¨¡å—ï¼Œç”¨äºç”ŸæˆåŒ—äº¬æ—¶é—´
from requests.adapters import HTTPAdapter  # å¯¼å…¥é€‚é…å™¨ï¼Œç”¨äºé…ç½®è¯·æ±‚é‡è¯•
from urllib3.util.retry import Retry     # å¯¼å…¥é‡è¯•ç­–ç•¥æ¨¡å—

# â€”â€” æ ¸å¿ƒé…ç½®åŒº â€”â€”
RULE_SOURCES = [
    {"name": "anti-AD", "url": "https://anti-ad.net/surge2.txt"},      # è§„åˆ™æº1ï¼šçº¯åŸŸåå¸¦ç‚¹æ ¼å¼
    {"name": "AdRules", "url": "https://adrules.top/adrules.list"}     # è§„åˆ™æº2ï¼šDOMAIN-SUFFIXæ ¼å¼
]
OUTPUT_FILE = "Loon_rules.txt"  # æœ€ç»ˆç”Ÿæˆçš„åˆå¹¶æ–‡ä»¶å

def create_retry_session():
    """åˆ›å»ºå¸¦è‡ªåŠ¨é‡è¯•æœºåˆ¶çš„ç½‘ç»œè¯·æ±‚ä¼šè¯ï¼Œå¢å¼ºç¨³å®šæ€§"""
    session = requests.Session()  # å®ä¾‹åŒ–ä¸€ä¸ªæ–°çš„è¯·æ±‚ä¼šè¯
    # é…ç½®é‡è¯•å‚æ•°ï¼šæ€»å…±é‡è¯•3æ¬¡ï¼Œé—´éš”æ—¶é—´éšé‡è¯•æ¬¡æ•°é€’å¢ï¼Œé’ˆå¯¹ç‰¹å®šçš„æœåŠ¡å™¨é”™è¯¯ç è¿›è¡Œé‡è¯•
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    session.mount("https://", HTTPAdapter(max_retries=retry))  # ä¸ºHTTPSåè®®æŒ‚è½½é‡è¯•ç­–ç•¥
    return session

def clean_domain(line):
    """
    æ ¸å¿ƒæ¸…æ´—å‡½æ•°ï¼šå°†ä¸åŒæ ¼å¼çš„è¡Œç»Ÿä¸€è½¬æ¢ä¸ºæ ‡å‡†çº¯åŸŸå
    æ”¯æŒå¤„ç†ï¼š.example.com å’Œ DOMAIN-SUFFIX,example.com
    """
    line = line.strip()  # å»é™¤æ¯ä¸€è¡Œå‰åçš„ç©ºæ ¼æˆ–æ¢è¡Œç¬¦
    
    # 1. æ’é™¤æ— æ•ˆè¡Œï¼šè·³è¿‡ç©ºè¡Œã€ä»¥ # æˆ– ! æˆ– [ å¼€å¤´çš„æ³¨é‡Š/æ ‡é¢˜è¡Œ
    if not line or line.startswith(('#', '!', '[')): 
        return None
    
    # 2. å¤„ç† AdRules æ ¼å¼ï¼šå¦‚æœåŒ…å«é€—å·ï¼Œè¯´æ˜å¸¦æœ‰ DOMAIN-SUFFIX æ ‡è®°
    if ',' in line:
        # ä½¿ç”¨é€—å·åˆ†å‰²ï¼Œå–é€—å·åé¢çš„éƒ¨åˆ†ï¼ˆå³çœŸå®çš„åŸŸåéƒ¨åˆ†ï¼‰
        parts = line.split(',')
        for part in parts:
            part = part.strip()
            # é€»è¾‘åˆ¤æ–­ï¼šåŒ…å«ç‚¹å·ä¸”ä¸æ˜¯å…¨å¤§å†™çš„æŒ‡ä»¤ï¼ˆå¦‚ REJECTï¼‰ï¼Œåˆ™è®¤å®šä¸ºåŸŸå
            if '.' in part and not part.isupper():
                line = part
                break
    
    # 3. ç»Ÿä¸€åŒ–å¤„ç†ï¼šå»æ‰åŸŸåæœ€å·¦ä¾§çš„åŸŸåç‚¹ï¼ˆä¾‹å¦‚ .google.com å˜ä¸º google.comï¼‰ï¼Œå¹¶è½¬ä¸ºå°å†™
    line = line.lstrip('.').lower()
    
    # 4. åˆæ³•æ€§æ ¡éªŒï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç¡®ä¿å‰©ä¸‹çš„å­—ç¬¦åªåŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸­åˆ’çº¿å’Œç‚¹å·
    if re.match(r'^[a-z0-9\-\.]+$', line) and '.' in line:
        return line
    return None

def compress_domains(domain_list):
    """
    å­åŸŸåå‹ç¼©ç®—æ³•ï¼šåç¼€åŒ¹é…å»é‡ï¼ˆDOMAIN-SET ä¸“å±ä¼˜åŒ–ï¼‰
    åŸç†ï¼šå¦‚æœå·²ç»æœ‰äº† google.comï¼Œé‚£ä¹ˆ ads.google.com å°±æ˜¯å¤šä½™çš„å†—ä½™è§„åˆ™
    """
    # æŒ‰åŸŸåé•¿åº¦ä»å°åˆ°å¤§æ’åºï¼Œä¼˜å…ˆå¤„ç†çŸ­çš„â€œæ ¹åŸŸåâ€
    sorted_domains = sorted(list(domain_list), key=len)
    final_list = []      # ç”¨äºå­˜å‚¨æœ€ç»ˆç²¾ç®€åçš„åŸŸååˆ—è¡¨
    redundant_count = 0  # å†—ä½™è®°å½•è®¡æ•°å™¨
    
    for domain in sorted_domains:
        is_redundant = False
        # å°†å½“å‰åŸŸåä¸å·²ç»å­˜å…¥æœ€ç»ˆåˆ—è¡¨çš„æ ¹åŸŸåè¿›è¡Œå¯¹æ¯”
        for root in final_list:
            # å¦‚æœå½“å‰åŸŸåä»¥â€œ.æ ¹åŸŸåâ€ç»“å°¾ï¼Œè¯´æ˜å®ƒæ˜¯è¯¥æ ¹åŸŸåçš„å­åŸŸåï¼Œå±äºå†—ä½™
            if domain.endswith('.' + root) or domain == root:
                is_redundant = True
                redundant_count += 1
                break
        if not is_redundant:
            final_list.append(domain)  # åªæœ‰éå†—ä½™çš„åŸŸåæ‰ä¼šè¢«ä¿ç•™
    return final_list, redundant_count

def main():
    """è„šæœ¬æ‰§è¡Œä¸»é€»è¾‘"""
    session = create_retry_session()  # è·å–å¸¦é‡è¯•åŠŸèƒ½çš„ä¼šè¯
    all_raw_domains = []             # æ±‡æ€»æ‰€æœ‰æºçš„åŸå§‹æœ‰æ•ˆåŸŸååˆ—è¡¨
    source_reports = []              # å­˜å‚¨æ¯ä¸ªæºçš„ç»Ÿè®¡æ•°æ®æŠ¥å‘Š
    global_stats = {"raw_lines": 0}  # ç»Ÿè®¡æ‰€æœ‰æºçš„æ€»è¡Œæ•°

    print("="*50 + "\nğŸš€ å¼€å§‹æ‰§è¡Œ Loon åŸŸåé›†åˆåˆå¹¶ä»»åŠ¡\n" + "="*50)
    
    for src in RULE_SOURCES:
        try:
            print(f"ğŸ“¥ æ­£åœ¨æ‹‰å–è§„åˆ™æºã€{src['name']}ã€‘...")
            resp = session.get(src['url'], timeout=30)  # å‘èµ·ç½‘ç»œè¯·æ±‚ä¸‹è½½æ–‡ä»¶
            lines = resp.text.splitlines()            # å°†ä¸‹è½½çš„å†…å®¹æŒ‰è¡Œåˆ‡åˆ†
            
            # æ¸…æ´—å½“å‰æºçš„æ‰€æœ‰è¡Œï¼Œå¹¶è¿‡æ»¤æ‰ None å€¼
            src_valid = [d for d in (clean_domain(l) for l in lines) if d]
            # æ‰§è¡Œæºå†…å»é‡ï¼Œç¡®ä¿å•ä¸ªæºæ–‡ä»¶å†…æ²¡æœ‰é‡å¤é¡¹
            unique_src = list(dict.fromkeys(src_valid))
            
            # è®°å½•å½“å‰æºçš„è¯¦ç»†ç»Ÿè®¡æ•°æ®
            source_reports.append({"name": src['name'], "raw": len(lines), "valid": len(src_valid)})
            all_raw_domains.extend(unique_src)  # å°†å¤„ç†åçš„åŸŸååŠ å…¥å…¨å±€æ±‡æ€»æ± 
            global_stats["raw_lines"] += len(lines)
            print(f"âœ… æ‹‰å–æˆåŠŸ: {src['name']} | æœ‰æ•ˆåŸŸåæ€»æ•°: {len(src_valid)}")
        except Exception as e:
            print(f"âŒ æ‹‰å–å¤±è´¥: {src['name']} | é”™è¯¯åŸå› : {e}")

    # ç¬¬ä¸€é˜¶æ®µå»é‡ï¼šè·¨æºå®Œå…¨é‡å¤é¡¹å»é‡
    unique_global = list(dict.fromkeys(all_raw_domains))
    cross_dedup = len(all_raw_domains) - len(unique_global)  # è®¡ç®—è·¨æºé‡å¤çš„æ•°é‡
    
    # ç¬¬äºŒé˜¶æ®µå»é‡ï¼šæ‰§è¡Œå­åŸŸååŒ…å«å…³ç³»å‹ç¼©ï¼ˆåç¼€åŒ¹é…ç²¾ç®€ï¼‰
    final_domains, compressed_count = compress_domains(unique_global)
    
    # è·å–å½“å‰åŒ—äº¬æ—¶é—´ (UTC+8) ç”¨äºå¤´éƒ¨æ³¨é‡Š
    beijing_time = (datetime.utcnow() + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S')

    # æ„å»ºç”Ÿæˆçš„è§„åˆ™æ–‡ä»¶å¤´éƒ¨æ³¨é‡Šä¿¡æ¯
    header = [
        f"# Loon åŸŸåé›†åˆ (DOMAIN-SET) åˆå¹¶ç‰ˆ",
        f"# ç”Ÿæˆæ—¶é—´: {beijing_time} (åŒ—äº¬æ—¶é—´)",
        f"# " + "="*40
    ]
    for r in source_reports:
        header.append(f"# æºã€{r['name']}ã€‘: æå–æœ‰æ•ˆè§„åˆ™ {r['valid']} æ¡")
    header.append(f"# è·¨æºå»é‡: {cross_dedup} æ¡ | å­åŸŸå†—ä½™å‹ç¼©: {compressed_count} æ¡")
    header.append(f"# æœ€ç»ˆåˆå¹¶æ€»æ•°: {len(final_domains)} æ¡")
    header.append(f"# " + "="*40 + "\n")

    # æœ€ç»ˆè¾“å‡ºï¼šç»Ÿä¸€ä¸ºå¸¦ç‚¹å‰ç¼€æ ¼å¼ï¼ˆ.example.comï¼‰ï¼ŒæŒ‰å­—æ¯é¡ºåºæ’åˆ—å¹¶å†™å…¥æ–‡ä»¶
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(header + [f".{d}" for d in sorted(final_domains)]))
    
    # åœ¨æ§åˆ¶å°æ‰“å°æœ€ç»ˆç»Ÿè®¡æ±‡æ€»ï¼Œæ–¹ä¾¿åœ¨ GitHub Actions æ—¥å¿—ä¸­ç›´æ¥æŸ¥çœ‹
    print(f"\nğŸ“Š æœ€ç»ˆåˆå¹¶ç»Ÿè®¡æ±‡æ€»:")
    print(f"- åŸå§‹æ€»è¡Œæ•°æ±‡æ€»: {global_stats['raw_lines']}")
    print(f"- è·¨æºé‡å¤é¡¹å·²å‰”é™¤: {cross_dedup}")
    print(f"- å­åŸŸå†—ä½™é¡¹å·²å‹ç¼©: {compressed_count}")
    print(f"- æœ€ç»ˆè¾“å‡ºæ–‡ä»¶æ€»é‡: {len(final_domains)}")
    print(f"ğŸš€ æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå·²ä¿å­˜ä¸º: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()  # è¿è¡Œä¸»ç¨‹åº
