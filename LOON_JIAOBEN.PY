#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import re
from datetime import datetime, timedelta
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# â€”â€” æ ¸å¿ƒé…ç½®åŒº â€”â€”
RULE_SOURCES = [
    {"name": "AdRules", "url": "https://adrules.top/adrules.list"},
    {"name": "anti-ad", "url": "https://anti-ad.net/surge2.txt"}
]
OUTPUT_FILE = "Loon_rules.txt"
# è‡ªåŠ¨é€‚é…ä½ çš„ Loon ä»“åº“åœ°å€
SUBSCRIBE_URL = "https://raw.githubusercontent.com/ddcm1349/Loon/main/Loon_rules.txt"

def create_retry_session():
    """åˆ›å»ºç¨³å®šè¯·æ±‚ä¼šè¯"""
    session = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    session.mount("https://", HTTPAdapter(max_retries=retry))
    return session

def clean_domain(line, source_name):
    """
    é«˜æ•ˆæ¸…æ´—é€»è¾‘ï¼š
    1. é’ˆå¯¹ anti-ad: è¯†åˆ«å¼€å¤´çš„ . ç›´æ¥æå–åŸŸåéƒ¨åˆ†
    2. é’ˆå¯¹ AdRules: è¯†åˆ« DOMAIN æˆ– DOMAIN-SUFFIX å¹¶æå–
    """
    line = line.strip()
    if not line or line.startswith(('#', '!', '[')): 
        return None

    # é€»è¾‘ Aï¼šå¦‚æœè¿™ä¸€è¡ŒåŒ…å«é€—å· (å¸¸è§äº AdRules çš„ DOMAIN, æˆ– DOMAIN-SUFFIX,)
    if ',' in line:
        parts = line.split(',')
        for part in parts:
            p = part.strip()
            # åªè¦åŒ…å«ç‚¹ä¸”ä¸æ˜¯çº¯å¤§å†™æŒ‡ä»¤ï¼Œå®ƒå°±æ˜¯åŸŸå
            if '.' in p and not p.isupper():
                line = p
                break
    
    # é€»è¾‘ Bï¼šå¤„ç†å¼€å¤´çš„ . (å¸¸è§äº anti-ad)
    line = line.lstrip('.')
    
    # ç»Ÿä¸€è½¬å°å†™å¹¶å»é™¤å¯èƒ½å­˜åœ¨çš„å¹²æ‰°
    line = line.lower().split('^')[0].split('/')[0]
    
    # æœ€ç»ˆéªŒè¯åˆæ³•æ€§
    if '.' in line and re.match(r'^[a-z0-9\-\.]+$', line):
        return line
    return None

def compress_domains(domain_list):
    """å­åŸŸåå†—ä½™å‹ç¼©ï¼šè‹¥æœ‰ abc.com åˆ™å‰”é™¤ sub.abc.com"""
    sorted_domains = sorted(list(domain_list), key=len)
    final_list = []
    redundant_count = 0
    for domain in sorted_domains:
        is_redundant = False
        for root in final_list:
            if domain.endswith('.' + root) or domain == root:
                is_redundant = True
                redundant_count += 1
                break
        if not is_redundant:
            final_list.append(domain)
    return final_list, redundant_count

def main():
    session = create_retry_session()
    source_stats = []
    all_valid_domains = []
    global_raw_total = 0

    print("="*50 + "\nğŸš€ å¼€å§‹æ‰§è¡Œ Loon è§„åˆ™é«˜æ•ˆåˆå¹¶\n" + "="*50)

    for src in RULE_SOURCES:
        try:
            print(f"ğŸ“¥ æ­£åœ¨æ‹‰å–ã€{src['name']}ã€‘...")
            resp = session.get(src['url'], timeout=30)
            lines = resp.text.splitlines()
            
            raw_count = len(lines)
            valid_list = []
            for l in lines:
                domain = clean_domain(l, src['name'])
                if domain:
                    valid_list.append(domain)
            
            # æºå†…å»é‡
            unique_src = list(dict.fromkeys(valid_list))
            inner_dedup = len(valid_list) - len(unique_src)
            
            source_stats.append({
                "name": src['name'],
                "url": src['url'],
                "raw": raw_count,
                "valid": len(valid_list),
                "unique": len(unique_src),
                "inner_dedup": inner_dedup
            })
            
            all_valid_domains.extend(unique_src)
            global_raw_total += raw_count
            print(f"âœ… ã€{src['name']}ã€‘æå–æˆåŠŸ: {len(unique_src)} æ¡")

        except Exception as e:
            print(f"âŒ ã€{src['name']}ã€‘æ‹‰å–å¤±è´¥: {e}")

    # å…¨å±€å»é‡ä¸å‹ç¼©
    unique_global = list(dict.fromkeys(all_valid_domains))
    cross_dedup = len(all_valid_domains) - len(unique_global)
    final_domains, compressed_count = compress_domains(unique_global)
    
    # æ—¶é—´åŒæ­¥
    beijing_time = (datetime.utcnow() + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S')

    # â€”â€” æ„å»ºè¯¦ç»†ç»Ÿè®¡æ³¨é‡Š â€”â€”
    header = [
        f"# Loon åŸŸåé›†åˆ (DOMAIN-SET) é«˜æ•ˆåˆå¹¶ç‰ˆ",
        f"# ç”Ÿæˆæ—¶é—´ (åŒ—äº¬æ—¶é—´) : {beijing_time}",
        f"# è§„åˆ™æºåœ°å€:",
    ]
    for i, s in enumerate(source_stats, 1):
        header.append(f"#   {i}. {s['name']} è§„åˆ™: {s['url']}")
    header.append(f"#   æœ¬è§„åˆ™è®¢é˜…åœ°å€: {SUBSCRIBE_URL}")
    header.append("# " + "="*72)
    
    header.append("# ã€å„è§„åˆ™æºå•ç‹¬ç»Ÿè®¡ã€‘")
    for i, s in enumerate(source_stats, 1):
        header.append(f"#   {i}. {s['name']} | åŸå§‹è¡Œ: {s['raw']} | æœ‰æ•ˆåŸŸå: {s['valid']} | æºå†…å»é‡: {s['inner_dedup']}")
    header.append("# " + "="*72)

    header.append("# ã€å…¨å±€åˆå¹¶ç»Ÿè®¡ã€‘")
    header.append(f"# åŸå§‹æ€»è¡Œæ•°æ±‡æ€»: {global_raw_total} æ¡")
    header.append(f"# è·¨æºé‡å¤å»é‡æ•°: {cross_dedup} æ¡ | å­åŸŸå†—ä½™å‹ç¼©æ•°: {compressed_count} æ¡")
    header.append(f"# æœ€ç»ˆå…¨å±€ä¿ç•™æ€»æ•°: {len(final_domains)} æ¡")
    header.append("# " + "="*72 + "\n")

    # å†™å…¥æ–‡ä»¶ï¼Œç»Ÿä¸€é‡‡ç”¨ . å‰ç¼€å®ç°é«˜æ•ˆåç¼€åŒ¹é…
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write('\n'.join(header))
        f.write('\n'.join([f".{d}" for d in sorted(final_domains)]))

    print(f"\nğŸ“Š ä»»åŠ¡å®Œæˆï¼æœ€ç»ˆä¿ç•™: {len(final_domains)} æ¡")

if __name__ == "__main__":
    main()
