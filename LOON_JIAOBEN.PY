#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests  # å¯¼å…¥ç½‘ç»œè¯·æ±‚åº“ï¼Œç”¨äºä¸‹è½½è¿œç¨‹è§„åˆ™
import re        # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“ï¼Œç”¨äºåŒ¹é…åŸŸåæ ¼å¼
from datetime import datetime, timedelta  # å¯¼å…¥æ—¶é—´æ¨¡å—ï¼Œç”¨äºç”ŸæˆåŒ—äº¬æ—¶é—´
from requests.adapters import HTTPAdapter  # å¯¼å…¥é€‚é…å™¨ï¼Œç”¨äºè®¾ç½®é‡è¯•ç­–ç•¥
from urllib3.util.retry import Retry     # å¯¼å…¥é‡è¯•é€»è¾‘æ¨¡å—

# â€”â€” é…ç½®åŒº â€”â€”
RULE_SOURCES = [
    {"name": "anti-AD", "url": "https://anti-ad.net/surge2.txt"},      # ç¬¬ä¸€ä¸ªè§„åˆ™æºï¼šanti-AD
    {"name": "AdRules", "url": "https://adrules.top/adrules.list"}     # ç¬¬äºŒä¸ªè§„åˆ™æºï¼šAdRules
]
OUTPUT_FILE = "Loon_rules.txt"  # æœ€ç»ˆç”Ÿæˆçš„æœ¬åœ°æ–‡ä»¶å

def create_retry_session():
    """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ä¼šè¯ï¼Œé˜²æ­¢ç½‘ç»œæ³¢åŠ¨å¯¼è‡´ä¸‹è½½å¤±è´¥"""
    session = requests.Session()  # å®ä¾‹åŒ–è¯·æ±‚ä¼šè¯
    # å®šä¹‰é‡è¯•è§„åˆ™ï¼šæ€»å…±é‡è¯•3æ¬¡ï¼Œé—´éš”1ç§’é€’å¢ï¼Œé’ˆå¯¹429/500/502/503/504çŠ¶æ€ç é‡è¯•
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
    session.mount("https://", HTTPAdapter(max_retries=retry))  # ä¸ºHTTPSè¯·æ±‚æŒ‚è½½é‡è¯•
    return session

def clean_domain(line):
    """æ¸…æ´—æ¯ä¸€è¡ŒåŸå§‹æ–‡æœ¬ï¼Œæå–çº¯åŸŸå"""
    line = line.strip()  # å»é™¤è¡Œé¦–å°¾ç©ºæ ¼
    # å¦‚æœæ˜¯ç©ºè¡Œã€æ³¨é‡Š(#æˆ–!)ã€æˆ–è€…æ˜¯Loonä¸è¯†åˆ«çš„æ ‡è®°ï¼Œç›´æ¥è·³è¿‡
    if not line or line.startswith(('#', '!', '[', 'PROCESS-NAME')): 
        return None
    
    # å¦‚æœè¿™ä¸€è¡ŒåŒ…å«é€—å·ï¼ˆä¾‹å¦‚ DOMAIN-SUFFIX,google.com,REJECTï¼‰
    if ',' in line:
        parts = line.split(',')  # æŒ‰é€—å·åˆ†å‰²å­—ç¬¦ä¸²
        for part in parts:
            part = part.strip()  # å»é™¤ç‰‡æ®µç©ºæ ¼
            # è¯†åˆ«åŸŸåï¼šåŒ…å«ç‚¹å·ä¸”ä¸æ˜¯çº¯å¤§å†™ï¼ˆæ’é™¤REJECTç­‰æŒ‡ä»¤ï¼‰
            if '.' in part and not part.isupper():
                line = part
                break
    
    line = line.strip('.').lower()  # ç»Ÿä¸€å»æ‰å‰åçš„ç‚¹å¹¶è½¬ä¸ºå°å†™
    # æ­£åˆ™æ ¡éªŒï¼šåªå…è®¸å­—æ¯ã€æ•°å­—ã€ä¸­åˆ’çº¿å’Œç‚¹å·
    return line if re.match(r'^[a-z0-9\-\.]+$', line) else None

def compress_domains(domain_list):
    """æ ¸å¿ƒå‹ç¼©ç®—æ³•ï¼šåç¼€åŒ¹é…å»é‡ï¼ˆæ ¸å¿ƒæ€§èƒ½ä¼˜åŒ–ï¼‰"""
    # æŒ‰ç…§åŸŸåå­—ç¬¦ä¸²é•¿åº¦ä»çŸ­åˆ°é•¿æ’åºï¼ˆä¾‹å¦‚å…ˆå¤„ç† google.comï¼Œå†å¤„ç† ads.google.comï¼‰
    sorted_domains = sorted(list(domain_list), key=len)
    final_list = []      # å­˜å‚¨å‹ç¼©åçš„æœ€ç»ˆåˆ—è¡¨
    redundant_count = 0  # å†—ä½™è®¡æ•°å™¨
    
    for domain in sorted_domains:
        is_redundant = False  # åˆå§‹æ ‡è®°ä¸ºéå†—ä½™
        for root in final_list:
            # å¦‚æœå½“å‰åŸŸåæ˜¯ä»¥â€œç‚¹+å·²å­˜åŸŸåâ€ç»“å°¾ï¼Œæˆ–è€…æ˜¯å·²å­˜åŸŸåæœ¬èº«
            # ä¾‹å¦‚ï¼šads.google.com ä»¥ .google.com ç»“å°¾ï¼Œè¯´æ˜å®ƒæ˜¯å†—ä½™çš„
            if domain.endswith('.' + root) or domain == root:
                is_redundant = True
                redundant_count += 1
                break
        if not is_redundant:
            final_list.append(domain)  # åªæœ‰éå†—ä½™çš„åŸŸåæ‰åŠ å…¥æœ€ç»ˆåå•
    return final_list, redundant_count

def main():
    """ä¸»æ‰§è¡Œé€»è¾‘"""
    session = create_retry_session()  # è·å–å¸¦é‡è¯•çš„è¯·æ±‚ä¼šè¯
    all_raw_domains = []             # å­˜å‚¨æ‰€æœ‰æºåˆå¹¶åçš„åŸå§‹åŸŸå
    source_reports = []              # å­˜å‚¨å„æºçš„ç»Ÿè®¡æŠ¥å‘Š
    global_stats = {"raw_lines": 0}  # å…¨å±€è¡Œæ•°ç»Ÿè®¡

    print("="*50 + "\nğŸš€ å¼€å§‹å¤„ç† Loon è§„åˆ™åˆå¹¶\n" + "="*50)
    
    for src in RULE_SOURCES:
        try:
            print(f"ğŸ“¥ æ­£åœ¨æ‹‰å–ã€{src['name']}ã€‘...")
            resp = session.get(src['url'], timeout=30)  # ä¸‹è½½è§„åˆ™
            lines = resp.text.splitlines()            # æŒ‰è¡Œåˆ‡åˆ†å†…å®¹
            # æ¸…æ´—æ¯ä¸€è¡Œï¼Œå¹¶å°†éç©ºçš„åŸŸåå­˜å…¥åˆ—è¡¨
            src_valid = [d for d in (clean_domain(l) for l in lines) if d]
            # æºå†…å»é‡ï¼ˆé˜²æ­¢å•æ–‡ä»¶å†…é‡å¤ï¼‰
            unique_src = list(dict.fromkeys(src_valid))
            
            # è®°å½•è¯¥æºç»Ÿè®¡æ•°æ®
            source_reports.append({"name": src['name'], "raw": len(lines), "valid": len(src_valid)})
            all_raw_domains.extend(unique_src)  # åŠ å…¥å…¨å±€å¤§æ± å­
            global_stats["raw_lines"] += len(lines)
            print(f"âœ… æ‹‰å–æˆåŠŸ: {src['name']} | æœ‰æ•ˆåŸŸå: {len(src_valid)}")
        except Exception as e:
            print(f"âŒ æ‹‰å–å¤±è´¥: {src['name']} | é”™è¯¯ä¿¡æ¯: {e}")

    # å…¨å±€è·¨æºå»é‡
    unique_global = list(dict.fromkeys(all_raw_domains))
    cross_dedup = len(all_raw_domains) - len(unique_global)  # è®¡ç®—è·¨æºé‡å¤äº†å¤šå°‘
    
    # æ‰§è¡Œå­åŸŸå‹ç¼©ç®—æ³•ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰
    final_domains, compressed_count = compress_domains(unique_global)
    
    # è®¡ç®—åŒ—äº¬æ—¶é—´ï¼ˆUTC+8ï¼‰
    beijing_time = (datetime.utcnow() + timedelta(hours=8)).strftime('%Y-%m-%d %H:%M:%S')

    # æ„å»ºæ–‡ä»¶å¤´éƒ¨æ³¨é‡Š
    header = [
        f"# Loon åŸŸåé›†åˆ (DOMAIN-SET)",
        f"# ç”Ÿæˆæ—¶é—´: {beijing_time}",
        f"# " + "="*30
    ]
    for r in source_reports:
        header.append(f"# {r['name']}: {r['valid']} æ¡")
    header.append(f"# è·¨æºå»é‡: {cross_dedup} | å­åŸŸå†—ä½™å‹ç¼©: {compressed_count}")
    header.append(f"# " + "="*30 + "\n")

    # å†™å…¥æ–‡ä»¶ï¼šç»Ÿä¸€åŠ ä¸Šç‚¹å·(.)å‰ç¼€ï¼Œå¹¶æŒ‰å­—æ¯æ’åº
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(header + [f".{d}" for d in sorted(final_domains)]))
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡åˆ° GitHub Action æ—¥å¿—ä¸­
    print(f"\nğŸ“Š ç»Ÿè®¡æ±‡æ€»:")
    print(f"- åŸå§‹æ€»è¡Œæ•°: {global_stats['raw_lines']}")
    print(f"- è·¨æºé‡å¤é¡¹: {cross_dedup}")
    print(f"- å­åŸŸå‹ç¼©é¡¹: {compressed_count}")
    print(f"- æœ€ç»ˆä¿ç•™æ•°: {len(final_domains)}")
    print(f"ğŸš€ æ–‡ä»¶å·²ä¿å­˜ä¸º: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()  # æ‰§è¡Œè„šæœ¬
